Space Knowledge Engine â€” Python Multi-Agent System

A modular multi-agent pipeline that ingests space-science text data, analyzes it, evaluates scientific relevance, summarizes key findings (optionally using Gemini), and stores refined knowledge in a persistent memory store.

The system is built to demonstrate real agent workflows: deliberate reasoning, tool use, long-term memory, deterministic evaluation, and clean orchestration â€” all running locally with optional API upgrades.

ğŸš€ Key Features
Fetcher Agent
Collects data from:

Local sample files (data/samples/*.txt)

Mock Arxiv feed (offline/demo mode)

Mock NASA API (offline/demo mode)

Analyzer Agent
Extracts structured information:

Word & sentence counts

Numeric values

Scientific measurements

Keyword matches

Scientific â€œclaimsâ€

Snippets for summarization

Evaluator Agent
Scores each item with transparent heuristics:

Keyword relevance

Numeric density

Measurement bonus

Claim strength

Length checks & penalties

Items passing the threshold flow deeper into the pipeline.

Summarizer Agent
Two modes:

Local deterministic summary

Gemini-powered summary (if enabled)

Memory Agent
Handles long-term storage:

Saves analysis + summaries

Deduplicates items

Compacts raw text to reduce footprint

Orchestrator Agent
The central controller:

Runs each cycle

Passes data between agents

Logs every run to data/demo_outputs/

Manages robust error handling + observability

Everything works completely offline, with API integrations available when desired.

ğŸ“‚ Project Structure

space-knowledge-engine/
â”‚
â”œâ”€â”€ agents/
â”‚   â”œâ”€â”€ orchestrator_agent.py
â”‚   â”œâ”€â”€ fetcher_agent.py
â”‚   â”œâ”€â”€ analyzer_agent.py
â”‚   â”œâ”€â”€ evaluator_agent.py
â”‚   â””â”€â”€ summarizer_agent.py
â”‚
â”œâ”€â”€ tools/
â”‚   â”œâ”€â”€ arxiv_fetcher.py
â”‚   â”œâ”€â”€ nasa_api.py
â”‚   â”œâ”€â”€ code_execution.py
â”‚   â””â”€â”€ parser_utils.py
â”‚
â”œâ”€â”€ data/
â”‚   â”œâ”€â”€ samples/
â”‚   â”‚   â””â”€â”€ example1.txt
â”‚   â”œâ”€â”€ demo_outputs/
â”‚   â”‚   â””â”€â”€ readme_demo_output.json
â”‚   â””â”€â”€ memory.json
â”‚
â”œâ”€â”€ docs/
â”‚   â”œâ”€â”€ agent_roles.md
â”‚   â”œâ”€â”€ architecture_diagram.png
â”‚   â””â”€â”€ workflow_diagram.png
â”‚
â”œâ”€â”€ main.py
â”œâ”€â”€ run_demo.py
â”œâ”€â”€ requirements.txt
â””â”€â”€ README.md

ğŸ”§ Installation
1. Create a virtual environment
Windows (PowerShell):

python -m venv venv
Set-ExecutionPolicy -Scope Process Bypass
.\venv\Scripts\activate

Mac/Linux:

python3 -m venv venv
source venv/bin/activate

2. Install dependencies

pip install -r requirements.txt

â–¶ï¸ Run the Demo
Runs a single end-to-end cycle:

python run_demo.py

Outputs written to:

data/demo_outputs/*.json

data/memory.json

â–¶ï¸ Continuous Mode
Runs multiple cycles with delays:

python main.py --iterations 3 --interval 1

ğŸ§  Optional: Enable Gemini Summarization
To switch summarization from rule-based â†’ Gemini:

1. Create a .env file in project root:

GEMINI_API_KEY=your_key_here

2. Enable Gemini in code

In summarizer_agent.py:

summarizer = SummarizerAgent(use_gemini=True)
If no API key is present, the system automatically falls back to the local summarizer.

ğŸ“˜ Documentation
Agent Roles: docs/agent_roles.md

Architecture Diagram: docs/architecture_diagram.png

Workflow Diagram: docs/workflow_diagram.png

ğŸ”­ Example Pipeline Run
Given an input file like:

data/samples/example1.txt

The pipeline:

Reads sample

Extracts measurements & keywords

Scores scientific relevance

Summarizes findings

Saves structured results to memory

ğŸ’¡ Future Work
If extended further, the system can support:

Real Arxiv & NASA API ingestion

PDF ingestion + table extraction

Vector embeddings for semantic memory search

Richer claim extraction using transformer models

A Streamlit dashboard to inspect processed knowledge

ğŸ“„ License
MIT License